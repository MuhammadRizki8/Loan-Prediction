# -*- coding: utf-8 -*-
"""Credit Risk Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jYklOJwjJxKC99TGWvyIY9c65VUADE5v

you are asked to build a model that can predict credit risk using a dataset provided by the company consisting of accepted and rejected loan data. In addition, you also need to prepare visual media to present the solution to the client. Make sure the visual media you create is clear, easy to read, and communicative. This end-to-end solution can be done in the Programming Language of your choice while still referring to the Data Science framework/methodology.

# IMPORT LIBRARIES
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import warnings
warnings.filterwarnings("ignore")

df=pd.read_csv("loan_data_2007_2014.csv")

df.head(3)

df.shape

"""the dataset dimensions have 466285 rows and 75 columns"""

df.info()

"""**Data Information**

- `rec` (Total Amount Committed): The total amount committed by investors for that loan at that point in time.
- `acc_now_delinq` (Number of Delinquent Accounts): The number of accounts on which the borrower is now delinquent.
- `addr_state` (Borrower's State): The state provided by the borrower in the loan application.
- `all_util` (Balance to Credit Limit): Balance to credit limit on all trades.
- `annual_inc` (Annual Income): The self-reported annual income provided by the borrower during registration.
- `annual_inc_joint` (Joint Annual Income): The combined self-reported annual income provided by the co-borrowers during registration.
- `application_type` (Application Type): Indicates whether the loan is an individual application or a joint application with two co-borrowers.
- `collection_recovery_fee` (Recovery Fee): Post-charge-off collection fee.
- `collections_12_mths_ex_med` (Collections in 12 Months): Number of collections in 12 months excluding medical collections.
- `delinq_2yrs` (Delinquencies in Past 2 Years): The number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years.
- `desc` (Loan Description): Loan description provided by the borrower.
- `dti_joint` (Joint Debt-to-Income Ratio): A ratio calculated using the co-borrowers' total monthly payments on the total debt obligations, excluding mortgages and the requested LC loan, divided by the co-borrowers' combined self-reported monthly income.
- `earliest_cr_line` (Earliest Credit Line): The month the borrower's earliest reported credit line was opened.
- `emp_length` (Employment Length): Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.
- `emp_title` (Employment Title): The job title supplied by the borrower when applying for the loan.
- `Femp` (Debt-to-Income Ratio): A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.
- `fico_range_high` (FICO Range High): The upper boundary range the borrower’s FICO at loan origination belongs to.
- `fico_range_low` (FICO Range Low): The lower boundary range the borrower’s FICO at loan origination belongs to.
- `funded_amnt` (Funded Amount): The total amount committed to that loan at that point in time.
- `grade` (Loan Grade): LC assigned loan grade.
- `home_ownership` (Home Ownership Status): The home ownership status provided by the borrower during registration. Our values are RENT, OWN, MORTGAGE, OTHER.
- `id` (Loan ID): A unique LC assigned ID for the loan listing.
- `il_util` (Installment Account Utilization): Ratio of total current balance to high credit/credit limit on all installment accounts.
- `initial_list_status` (Initial Listing Status): The initial listing status of the loan. Possible values are – Whole, Fractional.
- `inq_fi` (Personal Finance Inquiries): Number of personal finance inquiries.
- `inq_last_12m` (Inquiries in Past 12 Months): Number of credit inquiries in the past 12 months.
- `inq_last_6mths` (Inquiries in Past 6 Months): The number of inquiries in the past 6 months (excluding auto and mortgage inquiries).
- `installment` (Monthly Payment): The monthly payment owed by the borrower if the loan originates.
- `int_rate` (Interest Rate): Indicates if income was verified by LC, not verified, or if the income source was verified.
- `is_inc_v` (Income Verification): Indicates if income was verified.
- `issue_d` (Issue Date): The month in which the loan was funded.
- `last_fico_range_high` (Last FICO Range High): The upper boundary range the borrower’s last FICO pulled belongs to.
- `last_fico_range_low` (Last FICO Range Low): The lower boundary range the borrower’s last FICO pulled belongs to.
- `last_pymnt_amnt` (Last Payment Amount): Last total payment amount received.
- `last_pymnt_d` (Last Payment Date): Last month payment was received.
- `loan_amnt` (Loan Amount): The total amount funded for the loan.
- `loan_status` (Loan Status): Current status of the loan.
- `max_bal_bc` (Maximum Balance Owed): Maximum current balance owed on all revolving accounts.
- `member_id` (Borrower Member ID): A unique LC assigned ID for the borrower member.
- `mths_since_last_delinq` (Months Since Last Delinquency): The number of months since the borrower's last delinquency.
- `mths_since_last_major_derog` (Months Since Last Major Derogatory): Months since the most recent 90-day or worse rating.
- `mths_since_last_record` (Months Since Last Public Record): The number of months since the last public record.
- `mths_since_rcnt_il` (Months Since Most Recent Installment Account): Months since the most recent installment accounts were opened.
- `next_pymnt_d` (Next Payment Date): Next scheduled payment date.
- `open_acc` (Open Credit Lines): The number of open credit lines in the borrower's credit file.
- `open_acc_6m` (Open Trades in Last 6 Months): Number of open trades in the last 6 months.
- `open_il_12m` (Installment Accounts Opened in Last 12 Months): Number of installment accounts opened in the past 12 months.
- `open_il_24m` (Installment Accounts Opened in Last 24 Months): Number of installment accounts opened in the past 24 months.
- `open_il_6m` (Installment Accounts Opened in Last 6 Months): Number of installment accounts opened in the past 6 months.
- `open_rv_12m` (Revolving Trades Opened in Last 12 Months): Number of revolving trades opened in the past 12 months.
- `open_rv_24m` (Revolving Trades Opened in Last 24 Months): Number of revolving trades opened in the past 24 months.
- `out_prncp` (Outstanding Principal): Remaining outstanding principal for the total amount funded.
- `out_prncp_inv` (Outstanding Principal Invested): Remaining outstanding principal for the portion of the total amount funded by investors.
- `policy_code` (Policy Code): Publicly available policy code values are 1 for new products and 2 for products not publicly available.
- `pub_rec` (Derogatory Public Records): Number of derogatory public records.
- `purpose` (Loan Purpose): A category provided by the borrower for the loan request.
- `recoveries` (Recoveries): Indicates if a payment plan has been put in place for the loan.
- `revol_bal` (Revolving Balance): Total credit revolving balance.
"""

df.describe(include="object")

df.describe()

"""# Target Variable (Labelling)

The target variable is denoted by the column 'loan_status'. This target consists of several subtargets, and we will split it into good loan (0) and bad loan (1). Our goal is to build a model that has a high recall (1) score to minimize financial risk of the business.
"""

df.loan_status.unique()

df['good_bad'] = np.where(df.loc[:, 'loan_status'].isin(['Charged Off', 'Late (31-120 days)',
                                                                       'Late (16-30 days)', 'Default',
                                                                       'Does not meet the credit policy. Status:Charged Off'])
                                 , 1 , 0)

df.good_bad.value_counts()

"""0 = bad loan 1 = good loan"""

df.good_bad.value_counts(normalize=True)

x_label = ['Good Loan (0)', 'Bad Loan (1)']
loan_counts = df['good_bad'].value_counts()

# Membuat pie chart
plt.figure(figsize=(6, 6))
plt.title('Target Variable')
sns.set(style="whitegrid")
sns.color_palette("pastel")
plt.pie(loan_counts, labels=x_label, autopct='%1.1f%%', startangle=90)
plt.show()

"""There is data imbalance in the dataset, where there are about 89% good loan data and 11% bad loan data."""

# drop loan_stats
df.drop('loan_status', axis = 1, inplace = True)

"""## DATA CLEANING

checking categorical columns
"""

for col in df.select_dtypes(include= ['object','bool']).columns:
    print(col)
    print(df[col].unique())
    print()

"""identify some columns that need improvement"""

# Kolom/feature yang harus di cleaning
col_need_to_clean = ['term', 'emp_length', 'issue_d', 'earliest_cr_line', 'last_pymnt_d',
                    'next_pymnt_d', 'last_credit_pull_d']

"""#### term
Remove ' months' to '' then Convert data type to numeric
"""

df['term'] = pd.to_numeric(df['term'].str.replace(' months', ''))
df['term'].unique()

"""#### emp_length"""

df['emp_length'].unique()

df['emp_length'] = df['emp_length'].str.replace('\+ years', '')
df['emp_length'] = df['emp_length'].str.replace(' years', '')
df['emp_length'] = df['emp_length'].str.replace('< 1 year', str(0))
df['emp_length'] = df['emp_length'].str.replace(' year', '')

df['emp_length'].fillna(value = 0, inplace=True)
df['emp_length'] = pd.to_numeric(df['emp_length'])

df['emp_length'].unique()

"""#### date feature"""

col_date = ['issue_d', 'earliest_cr_line', 'last_pymnt_d',
                    'next_pymnt_d', 'last_credit_pull_d']

df[col_date].tail(3)

for col in col_date:
    df[col] = pd.to_datetime(df[col], format='%b-%y')

df[col_date].tail(3)

# Check apakah berhasil di cleaning
df[col_need_to_clean].info()

"""## MISSING VALUE

Features that have more than 50% missing values will be dropped, because if you want to fill it with other values such as median or mean, the error will be very high. It is better to drop it so as not to make the model even more inaccurate.
"""

missing_values = pd.DataFrame(df.isnull().sum()/df.shape[0])
missing_values = missing_values[missing_values.iloc[:,0] > 0.50]
missing_values.sort_values([0], ascending=False)

"""there are 21 columns that can be deleted"""

df.dropna(thresh = df.shape[0]*0.5, axis=1, inplace=True)

df.shape

"""# EDA (Exploratory Data Analysis)"""

df.sample()

def cat_stats(df, FEATURES):
    for feature in FEATURES:
        temp = df[feature].value_counts()
        df1 = pd.DataFrame({feature: temp.index, 'value': temp.values})
        cat_perc_0 = df[df['good_bad'] == 0].groupby(feature).size().reset_index(name='Count_Target_0')
        cat_perc_1 = df[df['good_bad'] == 1].groupby(feature).size().reset_index(name='Count_Target_1')
        cat_perc = cat_perc_0.merge(cat_perc_1, how='left', on=feature).fillna(0)
        cat_perc['Percentage_Target_0'] = cat_perc['Count_Target_0'] / (cat_perc['Count_Target_0'] + cat_perc['Count_Target_1']) * 100
        cat_perc['Percentage_Target_1'] = cat_perc['Count_Target_1'] / (cat_perc['Count_Target_0'] + cat_perc['Count_Target_1']) * 100
        cat_perc.sort_values(by=feature, inplace=True)

        fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20, 6))

        sns.set_color_codes("pastel")

        # Plot distribution for TARGET == 0
        sns.barplot(ax=ax1, x=feature, y="Percentage_Target_0", data=cat_perc)
        ax1.set_xticklabels(ax1.get_xticklabels(), rotation=50, fontsize=8)

        # Plot distribution for TARGET == 1
        sns.barplot(ax=ax2, x=feature, y='Percentage_Target_1', data=cat_perc)
        ax2.set_xticklabels(ax2.get_xticklabels(), rotation=50, fontsize=8)


        plt.tick_params(axis='both', which='major', labelsize=10)
        plt.show()

important_features = ['grade', 'sub_grade', 'purpose', 'addr_state', 'emp_length', 'home_ownership', 'verification_status', 'term']
cat_stats(df,important_features)

"""Based on the analysis, it appears that individuals with grade categories 'G', 'F', and 'E' are less likely to repay their loans, with subgrade G1 being the subgrade with the highest default rate. In addition, people who borrowed money to support a small business, move into a new home, or get married were less likely to repay the loan. Region of residence also plays an important role, with people in Hawaii, Delaware, and Alabama being less likely to repay loans, possibly due to the high cost of daily necessities in those regions. On the other hand, people in Iowa are more likely to repay their loans.

Employment is also a significant factor; people who have recently started working are less likely to repay loans. Housing status also comes into play, with people who do not own a home or who rent a place less likely to repay the loan. Account verification also has an impact, with people who have verified their accounts having a higher likelihood of repaying the loan.

In addition, the duration of the loan also plays an important role. People who chose the 60-month (5-year) option for their loan had a lower likelihood of repaying their loan, possibly due to the length of the loan. All of these findings provide valuable insights into understanding loan repayment patterns and can be used for wiser lending decisions in the future.

### Highly correlated column
"""

mask = np.zeros_like(df.corr().fillna(0), dtype = np.bool)
mask[np.triu_indices_from(mask)] = True
plt.figure(figsize=(20, 20))
sns.heatmap(df.corr(), mask = mask, annot = True,  cmap = "inferno", vmin = -1, fmt = '.1g', edgecolor = 'w', linewidth = 0.6)
plt.show()

"""Here, if there are pairs of features that have a high correlation, one of them will be taken. The correlation value used as a benchmark for high correlation is uncertain, generally 0.7 is used."""

corr_matrix = df.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
to_drop_hicorr = [column for column in upper.columns if any(upper[column] >= 0.7)]

to_drop_hicorr

df.drop(to_drop_hicorr, axis=1, inplace=True)

df.shape

"""### Cardinality

At this stage, features that have a very high unique value (high cardinality) and features that have only one unique value are removed.
"""

df.select_dtypes(include='object').nunique()

df.drop(['emp_title', 'url', 'title', 'application_type'], axis = 1, inplace = True)

df.select_dtypes(exclude = 'object').nunique()

df.drop(['Unnamed: 0', 'id', 'policy_code'], axis = 1, inplace = True)

"""### dominant category class feature"""

# the percentage distribution of unique values in each column of object data type (string or category) in a DataFramedata
for col in df.select_dtypes(include='object').columns.tolist():
    print(df[col].value_counts(normalize=True)*100)
    print('\n')

"""Features that are heavily dominated by only one of the values will be discarded at this stage."""

df.drop('pymnt_plan', axis = 1, inplace = True)

df.shape

"""## Filling missing value"""

df.isna().sum()

columns_with_null = ['annual_inc', 'delinq_2yrs', 'earliest_cr_line', 'inq_last_6mths', 'open_acc',
                     'pub_rec', 'revol_util', 'total_acc', 'last_pymnt_d', 'next_pymnt_d','last_credit_pull_d', 'collections_12_mths_ex_med',
                     'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal']

num_rows = len(columns_with_null) // 2 + len(columns_with_null) % 2
fig, axes = plt.subplots(num_rows, 2, figsize=(16, 4 * num_rows))
for i, column in enumerate(columns_with_null):
    row = i // 2
    col = i % 2
    sns.histplot(df[column].dropna(), bins=30, kde=True, color='skyblue', ax=axes[row, col])
    axes[row, col].set_xlabel(column)
    axes[row, col].set_ylabel('Frequency')
    axes[row, col].set_title(f'Distribution of {column}')

if len(columns_with_null) % 2 != 0:
    fig.delaxes(axes[num_rows - 1, 1])

plt.tight_layout()
plt.show()

def fill_missing_values(data, columns_with_null):
    # Loop melalui kolom-kolom dengan nilai null dan mengisi nilai null berdasarkan tipe data kolomnya
    for column in columns_with_null:
        if data[column].dtype == 'object':
            # Jika tipe data kolom adalah object/kategori, isi dengan modus
            data[column].fillna(data[column].mode()[0], inplace=True)
        else:
            # Jika tipe data kolom adalah numerik, isi dengan median
            data[column].fillna(data[column].median(), inplace=True)

    return data
# Mengisi nilai null pada X_train
df = fill_missing_values(df, columns_with_null)

df.isna().sum()

for col in df.select_dtypes(exclude= ['int64', 'float64']).columns:
    print(col)
    print(df[col].unique())
    print()

"""# FEATURE ENGINEERING"""

df.shape

from datetime import date

date.today().strftime('%Y-%m-%d')

"""Generally, reference date = today is used. However, since this dataset is a 2007-2014 dataset, it is more relevant to use a reference date around 2017. In this example, 2017-12-01 is used as the reference date."""

df.sample()

# feature engineering untuk date columns
def date_columns(df, column):
    today_date = pd.to_datetime('2017-12-01')
    df[column] = pd.to_datetime(df[column], format = "%b-%y")
    df['mths_since_' + column] = round(pd.to_numeric((today_date - df[column]) / np.timedelta64(1, 'M')))
#     df.drop(columns = [column], inplace=True)

# apply to X_train
for col in col_date:
    date_columns(df, col)
# date_columns(X_train, 'earliest_cr_line')
# date_columns(X_train, 'issue_d')
# date_columns(X_train, 'last_pymnt_d')
# date_columns(X_train, 'last_credit_pull_d')

for col in col_date:
    print(df['mths_since_' + col].describe())
    print("-----------------------------")

"""handling negative value anomalies"""

df[df['mths_since_earliest_cr_line']<0][['earliest_cr_line', 'mths_since_earliest_cr_line']]

"""there are some years that don't make sense.
The negative value arises because the Python function misinterprets the year 62 to be 2062, when it should be 1962.

To solve this, simply turn the negative value into a maximum value. Since the negative values here mean old data (1900s), it still makes sense if I change the values to the largest value.
"""

df.loc[df['mths_since_earliest_cr_line']<0, 'mths_since_earliest_cr_line'] = df['mths_since_earliest_cr_line'].max()

df = df.drop(columns=df.select_dtypes(include=['datetime64']).columns)
df.info()

df.shape

"""# Feature Selection"""

df.sample()

for col in df.select_dtypes(include= ['object','bool']).columns:
    print(col)
    print(df[col].unique())
    print()

for col in df.select_dtypes(exclude= ['object','bool']).columns:
    print(col)
    print(df[col].unique())
    print()

cols_to_drop = [
    # beberapa kolom yang tidak berguna untuk pemodelan
    'zip_code'
    , 'grade'
    , 'mths_since_last_pymnt_d'
    , 'mths_since_next_pymnt_d'
    , 'mths_since_last_credit_pull_d'
]

df = df.drop(cols_to_drop, axis=1)

df.info()

numerical_cols=df.select_dtypes(exclude= ['object','bool']).columns.tolist()
for col in numerical_cols:
    plt.figure(figsize=(12, 6))
    plt.subplots_adjust(wspace=0.4)
    plt.subplot(1, 2, 1)
    sns.histplot(df[col].dropna(), bins=30, color='skyblue', edgecolor='black')
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')

    plt.subplot(1, 2, 2)
    sns.boxplot(x='good_bad', y=col, data=df, palette='pastel')
    plt.title(f'Boxplot of {col} by Loan Status')
    plt.xlabel('Loan Status')
    plt.ylabel(col)

    plt.show()

categorical_columns = df.select_dtypes(include=['object']).columns.tolist()
plt.figure(figsize=(15, 10))

for i, column in enumerate(categorical_columns, start=1):
    plt.subplot(2, 3, i)
    sns.countplot(x=column, data=df, palette='Set1')
    plt.title(column)
    plt.xlabel('')
    plt.ylabel('')
    plt.xticks(rotation=90)

plt.tight_layout()
plt.show()

"""# SCALING & TRANSFORMATION"""

encoded_cols = [col for col in df.select_dtypes(include = 'object').columns.tolist()]
onehot = pd.get_dummies(df[encoded_cols], drop_first = True)

onehot.head()

num_cols = [col for col in df.columns.tolist() if col not in encoded_cols + ['good_bad']]
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
std = pd.DataFrame(ss.fit_transform(df[num_cols]), columns = num_cols)

std.head()

data_model = pd.concat([onehot, std, df[['good_bad']]], axis = 1)
data_model.head(3)

"""# MODELING

## Split Data
"""

from sklearn.model_selection import train_test_split
X = data_model.drop('good_bad', axis=1)
y = data_model['good_bad']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

plt.title('Bad (1) vs Good (0) Loans Balance')
sns.barplot(x=data_model.good_bad.value_counts().index,y=data_model.good_bad.value_counts().values)

y_train.value_counts(normalize=True)*100

"""this dataset has imbalanced data.

# Training
"""

# Import Library
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_recall_curve
# training
LR= LogisticRegression(max_iter=600).fit(X_train, y_train)
# predicting
y_pred_LR = LR.predict(X_test)

# classification report
target_names = ['good loan', 'bad loan']
print('Classification_Report:')
print(classification_report(y_test, y_pred_LR, digits=4, target_names = target_names))

from sklearn.ensemble import RandomForestClassifier
model_rfc = RandomForestClassifier(max_depth=4)
model_rfc.fit(X_train, y_train)
y_pred_rfc = model_rfc.predict(X_test)

# classification report
target_names = ['good loan', 'bad loan']
print('Classification_Report:')
print(classification_report(y_test, y_pred_rfc, digits=4, target_names = target_names))

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
cm = confusion_matrix(y_test, y_pred_LR)
sns.heatmap(cm, annot=True, fmt='.0f', cmap=plt.cm.Blues)
plt.xlabel('y_pred')
plt.ylabel('y_test')
plt.show()

"""- Logistic Regression:
    This model performed much better than Random Forest. The high precision in the "bad loan" category shows that this model is very good at identifying high-risk loans. However, since the recall is also very high, this model may be too strict and discard too many actual low-risk loans.


- Random Forest:
    This model has perfect precision for the "good loan" category, but very low recall. This suggests that while this model can identify low-risk loans very well (high precision), it fails to identify most of the actual low-risk loans (low recall).

especially the very low recall for minority classes, this model cannot be considered good for this case. The model achieved high accuracy by predicting only the majority class, but failed to capture the minority class,

This is due to the unbalanced dataset that the machine learning model ignores the minority class (bad loan class) completely.

Thus, this class imbalance may affect the model during training. This is a problem because bad loan data (minority class) is needed for this prediction model.
The minority class oversampling technique will be used to overcome this data imbalance.

## Oversampling Minority Class to Resolve Class Imbalance

Random Over Sampling

Random oversampling involves randomly duplicating examples from the minority class and adding them to the training dataset.
"""

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler()
X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)

print('Before OverSampling:\n{}'.format(y_train.value_counts()))
print('\nAfter OverSampling:\n{}'.format(y_train_ros.value_counts()))

y_train_ros.value_counts(normalize=True)

"""## Train model after resampling

### Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier

LR_ros= LogisticRegression(max_iter=600)
LR_ros.fit(X_train_ros, y_train_ros)

#predicting
y_pred_LR_ros = LR_ros.predict(X_test)

#classification report
print('Classification_Report:')
print(classification_report(y_test, y_pred_LR_ros, digits=4))

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
cm = confusion_matrix(y_test, y_pred_LR_ros)
sns.heatmap(cm, annot=True, fmt='.0f', cmap=plt.cm.Blues)
plt.xlabel('y_pred')
plt.ylabel('y_test')
plt.show()

"""### Decision Tree"""

dt_ros = DecisionTreeClassifier(max_depth = 10)
dt_ros.fit(X_train_ros, y_train_ros)

#predicting
y_pred_dt_ros = dt_ros.predict(X_test)

#classification report
print('Classification_Report:')
print(classification_report(y_test, y_pred_dt_ros, digits=4))

"""### Random Forest"""

rf_ros = RandomForestClassifier(max_depth=10, n_estimators=20)
rf_ros.fit(X_train_ros, y_train_ros)

#predicting
y_pred_rf_ros = rf_ros.predict(X_test)

#classification report
print('Classification_Report:')
print(classification_report(y_test, y_pred_rf_ros, digits=4))

"""### XGBoost"""

from xgboost import XGBClassifier
xgb_ros = XGBClassifier(max_depth=5)
xgb_ros.fit(X_train_ros, y_train_ros)

#predicting
y_pred_xgb_ros = xgb_ros.predict(X_test)

#classification report
print('Classification_Report:')
print(classification_report(y_test, y_pred_xgb_ros, digits=4))

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
cm = confusion_matrix(y_test, y_pred_xgb_ros)
sns.heatmap(cm, annot=True, fmt='.0f', cmap=plt.cm.Blues)
plt.xlabel('y_pred')
plt.ylabel('y_test')
plt.show()

"""The best average accuracy result among all the models above is using XGBoost with an average accuracy value of 87%. This model has better performance in identifying high-risk loans (high precision and good recall)."""

arr_feature_importances = xgb_ros.feature_importances_
arr_feature_names = X_train.columns.values
df_feature_importance = pd.DataFrame(index=range(len(arr_feature_importances)), columns=['feature', 'importance'])
df_feature_importance['feature'] = arr_feature_names
df_feature_importance['importance'] = arr_feature_importances
df_all_features = df_feature_importance.sort_values(by='importance', ascending=False)
df_all_features

top_features = df_all_features.head(10)

plt.figure(figsize=(12, 10))
plt.barh(top_features['feature'], top_features['importance'], color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Top 10 Features Importance')
plt.gca().invert_yaxis()
plt.show()

"""# Validation

measuring model performance using AUC (abbrevation for area under the curve) & KS (Kolmogorov-Smirnov)
"""

y_pred_prob = xgb_ros.predict_proba(X_test)[:][:,1]
df_act_pred = pd.concat([pd.DataFrame(np.array(y_test), columns = ['y_actual']), pd.DataFrame(y_pred_prob, columns = ['y_pred_prob'])], axis = 1)
df_act_pred.index = y_test.index

"""### AUC (Abbrevation for Area Under the Curve)"""

from sklearn.metrics import roc_curve, roc_auc_score
fpr, tpr, tr = roc_curve(df_act_pred['y_actual'], df_act_pred['y_pred_prob'])
auc = roc_auc_score(df_act_pred['y_actual'], df_act_pred['y_pred_prob'])

plt.plot(fpr, tpr, label = 'AUC  = %0.4f' %auc)
plt.plot(fpr, fpr, linestyle = '--', color = 'k')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()

"""## KS (Kolmogorov-Smirnov)"""

df_act_pred = df_act_pred.sort_values('y_pred_prob')
df_act_pred = df_act_pred.reset_index()
df_act_pred['Cumulative N Population'] = df_act_pred.index + 1
df_act_pred['Cumulative N Bad'] = df_act_pred['y_actual'].cumsum()
df_act_pred['Cumulative N Good'] = df_act_pred['Cumulative N Population'] - df_act_pred['Cumulative N Bad']
df_act_pred['Cumulative Perc Population'] = df_act_pred['Cumulative N Population'] / df_act_pred.shape[0]
df_act_pred['Cumulative Perc Bad'] = df_act_pred['Cumulative N Bad'] / df_act_pred['y_actual'].sum()
df_act_pred['Cumulative Perc Good'] = df_act_pred['Cumulative N Good'] / (df_act_pred.shape[0] - df_act_pred['y_actual'].sum())

df_act_pred.head()

KS = max(df_act_pred['Cumulative Perc Good'] - df_act_pred['Cumulative Perc Bad'])

plt.plot(df_act_pred['y_pred_prob'], df_act_pred['Cumulative Perc Bad'], color = 'r')
plt.plot(df_act_pred['y_pred_prob'], df_act_pred['Cumulative Perc Good'], color = 'b')
plt.xlabel('Estimated Probability for Being Bad')
plt.ylabel('Cumulative %')
plt.title('Kolmogorov-Smirnov : %0.4f' %KS)

"""# Conclusion

- AUC (Area Under the ROC Curve):
<br>AUC is a measure of how well the model can distinguish between positive and negative classes. The higher the AUC value, the better the model is at correctly predicting the positive class. In the context of credit risk, this means the model's ability to distinguish between borrowers who repay their loans well and borrowers who default.
> AUC = 0.904: This value indicates that the model has a fairly good ability to distinguish between good and bad borrowers.


- KS (Kolmogorov-Smirnov):
<br>KS is the difference between two cumulative distribution functions: a positive cumulative distribution function (positive CDF) for the high-risk group and a negative cumulative distribution function (negative CDF) for the low-risk group. The maximum KS value indicates that the model can separate the high and low risk groups well.
> KS = 0.63: A high KS value indicates that the model can distinguish well between the high and low risk groups, with the greatest difference at the point where sensitivity and specificity reach a maximum.

In the world of credit risk modeling, generally AUC above 0.7 and KS above 0.3 are considered good performance.
"""